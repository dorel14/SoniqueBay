"""
Script de test pour v√©rifier le service LLM unifi√© (Ollama/KoboldCPP).
Auteur: SoniqueBay Team
"""
import asyncio
import sys
import os

# Ajouter le backend au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from backend.api.services.llm_service import LLMService, llm_service


def test_health_check():
    """Teste la v√©rification de sant√© du service LLM."""
    print("\n=== Test Health Check ===")
    health = llm_service.health_check()
    print(f"Statut: {health['status']}")
    print(f"Fournisseur: {health['provider']}")
    print(f"URL: {health['base_url']}")
    if 'error' in health:
        print(f"Erreur: {health['error']}")
    return health['status'] == 'healthy'


def test_model_list():
    """Teste la r√©cup√©ration de la liste des mod√®les."""
    print("\n=== Test Liste des Mod√®les ===")
    models = llm_service.get_model_list()
    print(f"Nombre de mod√®les: {len(models.get('models', []))}")
    for model in models.get('models', [])[:3]:  # Afficher les 3 premiers
        print(f"  - {model.get('name', 'N/A')}")
    return len(models.get('models', [])) > 0


async def test_chat_response():
    """Teste la g√©n√©ration d'une r√©ponse de chat."""
    print("\n=== Test R√©ponse Chat ===")
    try:
        messages = [
            {"role": "system", "content": "Tu es un assistant musical. Sois concis."},
            {"role": "user", "content": "Bonjour, peux-tu me recommander de la musique ?"}
        ]
        
        response = await llm_service.generate_chat_response(
            messages=messages,
            temperature=0.7,
            max_tokens=256,
            stream=False
        )
        
        print(f"R√©ponse: {response.get('content', 'Pas de r√©ponse')[:100]}...")
        print(f"Mod√®le utilis√©: {response.get('model', 'N/A')}")
        return bool(response.get('content'))
        
    except Exception as e:
        print(f"Erreur: {e}")
        return False


def test_provider_detection():
    """Teste la d√©tection automatique du fournisseur."""
    print("\n=== Test D√©tection Fournisseur ===")
    service = LLMService(provider_type='auto')
    print(f"Fournisseur d√©tect√©: {service.provider_type}")
    print(f"URL: {service.base_url}")
    return service.provider_type in ['koboldcpp', 'ollama']


async def main():
    """Fonction principale de test."""
    print("=" * 60)
    print("Test du Service LLM Unifi√© (Ollama/KoboldCPP)")
    print("=" * 60)
    
    results = []
    
    # Test 1: D√©tection du fournisseur
    results.append(("D√©tection Fournisseur", test_provider_detection()))
    
    # Test 2: Health check
    results.append(("Health Check", test_health_check()))
    
    # Test 3: Liste des mod√®les
    results.append(("Liste Mod√®les", test_model_list()))
    
    # Test 4: R√©ponse chat
    results.append(("R√©ponse Chat", await test_chat_response()))
    
    # R√©sum√©
    print("\n" + "=" * 60)
    print("R√âSUM√â DES TESTS")
    print("=" * 60)
    
    for name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{name}: {status}")
    
    total = len(results)
    passed = sum(1 for _, r in results if r)
    print(f"\nTotal: {passed}/{total} tests r√©ussis")
    
    if passed == total:
        print("\nüéâ Tous les tests ont r√©ussi ! Le service LLM est correctement configur√©.")
        return 0
    else:
        print("\n‚ö†Ô∏è  Certains tests ont √©chou√©. V√©rifiez la configuration.")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

